{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "38d16ee7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules from repository\n",
    "from model.data.pipeline import utils as ut\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3031ba1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe of all monthly data\n",
    "df = ut.load_sql_as_df('SELECT * From MonthlyDataModel;')\n",
    "\n",
    "#dataframe of month averages to fill in bad values\n",
    "monthAvg = ut.load_sql_as_df('SELECT * From MonthlyAverages;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "# Getting features and choosing labels. Fill NaN spots with NaN text to manipulate them\n",
    "labels = df[\"state\"]\n",
    "#['region', 'latitude', 'longitude', 'elevation', 'region'] are all dropped automatically\n",
    "df = df.fillna('NaN')\n",
    "\n",
    "# Changing columns in to months to help compare\n",
    "featSer = df[\"month\"].to_numpy()\n",
    "prcpIntM = monthAvg[\"prcpInt\"].to_numpy()\n",
    "prcpFreqM = monthAvg[\"prcpFreq\"].to_numpy()\n",
    "temp_max_normalM = monthAvg[\"temp_max_normal\"].to_numpy()\n",
    "temp_min_normalM = monthAvg[\"temp_min_normal\"].to_numpy()\n",
    "snow_depthM = monthAvg[\"snowInt\"].to_numpy()\n",
    "snow_daysM = monthAvg[\"snowFreq\"].to_numpy()\n",
    "cloudsM = monthAvg[\"clouds\"].to_numpy()\n",
    "dewM = monthAvg[\"dew_point\"].to_numpy()\n",
    "heatM = monthAvg[\"heat_index\"].to_numpy()\n",
    "pressureM = monthAvg[\"pressure\"].to_numpy()\n",
    "windM = monthAvg[\"wind_speed\"].to_numpy()\n",
    "windCalmM = monthAvg[\"wind_calm_percentage\"].to_numpy()\n",
    "\n",
    "prcp_normRay = df[\"prcp_normal\"].to_numpy()\n",
    "prcp_days_tRay = df[\"prcp_days_t\"].to_numpy()\n",
    "temp_max_normalRay = df[\"temp_max_normal\"].to_numpy()\n",
    "temp_min_normalRay = df[\"temp_min_normal\"].to_numpy()\n",
    "snow_depthRay = df[\"snow_depth_days\"].to_numpy()\n",
    "snow_daysRay = df[\"snow_days_t\"].to_numpy()\n",
    "cloudsRay = df[\"clouds\"].to_numpy()\n",
    "dewRay = df[\"dew_point\"].to_numpy()\n",
    "heatRay = df[\"heat_index\"].to_numpy()\n",
    "pressureRay = df[\"pressure\"].to_numpy()\n",
    "windRay = df[\"wind_speed\"].to_numpy()\n",
    "windCalmRay = df[\"wind_calm_percentage\"].to_numpy()\n",
    "\n",
    "#replacing bad values\n",
    "for i in range(len(featSer)):\n",
    "    if prcp_days_tRay[i] == 'NaN' or prcp_days_tRay[i] == -9999 or prcp_days_tRay[i] == -8888 or prcp_days_tRay[\n",
    "        i] == -7777 or prcp_days_tRay[i] == -6666 or prcp_days_tRay[i] == -5555:\n",
    "        prcp_days_tRay[i] = prcpFreqM[featSer[i] - 1]\n",
    "    if temp_max_normalRay[i] == 'NaN' or temp_max_normalRay[i] == -9999 or temp_max_normalRay[i] == -8888 or temp_max_normalRay[i] == -7777 or temp_max_normalRay[i] == -6666 or temp_max_normalRay[i] == -5555:\n",
    "        temp_max_normalRay[i] = temp_max_normalM[featSer[i] - 1]\n",
    "    if temp_min_normalRay[i] == 'NaN' or temp_min_normalRay[i] == -9999 or temp_min_normalRay[i] == -8888 or temp_min_normalRay[i] == -7777 or temp_min_normalRay[i] == -6666 or temp_min_normalRay[i] == -5555:\n",
    "        temp_min_normalRay[i] = temp_min_normalM[featSer[i] - 1]\n",
    "    if snow_daysRay[i] == 'NaN' or snow_daysRay[i] == -9999 or snow_daysRay[i] == -8888 or snow_daysRay[i] == -7777 or snow_daysRay[i] ==-6666 or snow_daysRay[i] == -5555:\n",
    "        snow_daysRay[i] = snow_daysM[featSer[i] - 1]\n",
    "    if cloudsRay[i] == 'NaN' or cloudsRay[i] == -9999 or cloudsRay[i] == -8888 or cloudsRay[i] == -7777 or cloudsRay[i] == -6666 or cloudsRay[i] == -5555:\n",
    "        cloudsRay[i] = cloudsM[featSer[i] - 1]\n",
    "    if dewRay[i] == 'NaN' or dewRay[i] == -9999 or dewRay[i] == -8888 or dewRay[i] == -7777 or dewRay[i] == -6666 or dewRay[i] == -5555:\n",
    "        dewRay[i] = dewM[featSer[i] - 1]\n",
    "    if heatRay[i] == 'NaN' or heatRay[i] == -9999 or heatRay[i] == -8888 or heatRay[i] == -7777 or heatRay[i] == -6666 or heatRay[i] == -5555:\n",
    "        heatRay[i] = heatM[featSer[i] - 1]\n",
    "    if windRay[i] == 'NaN' or windRay[i] == -9999 or windRay[i] == -8888 or windRay[i] == -7777 or windRay[i] == -6666 or windRay[i] == -5555:\n",
    "        windRay[i] = windM[featSer[i] - 1]\n",
    "\n",
    "df1 = df.copy()\n",
    "# Recreating filled dataset\n",
    "df = pd.DataFrame({'state': labels, 'month' : featSer, 'prcp_days_tRay': prcp_days_tRay,\n",
    "                   \"temp_max_normalRay\": temp_max_normalRay, \"temp_min_normalRay\": temp_min_normalRay,\n",
    "                   \"snow_daysRay\": snow_daysRay, \"cloudsRay\": cloudsRay,\n",
    "                   \"dewRay\": dewRay, \"heatRay\": heatRay, \"windRay\": windRay})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "       state  month prcp_days_tRay temp_max_normalRay temp_min_normalRay  \\\n0         AL      1           69.0          41.924644          21.303576   \n1         AL      2           65.0          45.985372          24.106816   \n2         AL      3           62.0          54.495133          31.252366   \n3         AL      4           48.0          64.407005          39.586609   \n4         AL      5           48.0          73.406937           48.77541   \n...      ...    ...            ...                ...                ...   \n117031    NE      8      25.909684               85.4               62.7   \n117032    NE      9      46.810539               77.8               53.6   \n117033    NE     10      47.409751               65.2               40.6   \n117034    NE     11      49.541899               49.7               28.0   \n117035    NE     12      50.036534               36.9               17.1   \n\n       snow_daysRay  cloudsRay     dewRay    heatRay   windRay longitude_bin  \n0               0.0  55.667977  26.341895  35.417982  8.593669             3  \n1               0.0  55.013862  28.228661  38.559721  8.813964             3  \n2       -297.336583  52.051446  33.052022  45.700434  9.329369             3  \n3               0.0  49.932187   39.67777  54.337107  9.386922             3  \n4               0.0  48.941521  48.669454    63.3096  8.659783             3  \n...             ...        ...        ...        ...       ...           ...  \n117031   -32.307496  40.020323  59.955251  74.535821  7.050787             3  \n117032  -337.711363  41.015647  53.890063  67.271141  7.402396             3  \n117033  -656.829311  45.018233   44.25134  56.376711  7.768246             3  \n117034  -363.569282   51.45818  35.445112  45.895488  8.266765             3  \n117035  -291.094614  55.725457   28.84527  37.656348  8.419217             3  \n\n[117036 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>state</th>\n      <th>month</th>\n      <th>prcp_days_tRay</th>\n      <th>temp_max_normalRay</th>\n      <th>temp_min_normalRay</th>\n      <th>snow_daysRay</th>\n      <th>cloudsRay</th>\n      <th>dewRay</th>\n      <th>heatRay</th>\n      <th>windRay</th>\n      <th>longitude_bin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AL</td>\n      <td>1</td>\n      <td>69.0</td>\n      <td>41.924644</td>\n      <td>21.303576</td>\n      <td>0.0</td>\n      <td>55.667977</td>\n      <td>26.341895</td>\n      <td>35.417982</td>\n      <td>8.593669</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AL</td>\n      <td>2</td>\n      <td>65.0</td>\n      <td>45.985372</td>\n      <td>24.106816</td>\n      <td>0.0</td>\n      <td>55.013862</td>\n      <td>28.228661</td>\n      <td>38.559721</td>\n      <td>8.813964</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AL</td>\n      <td>3</td>\n      <td>62.0</td>\n      <td>54.495133</td>\n      <td>31.252366</td>\n      <td>-297.336583</td>\n      <td>52.051446</td>\n      <td>33.052022</td>\n      <td>45.700434</td>\n      <td>9.329369</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AL</td>\n      <td>4</td>\n      <td>48.0</td>\n      <td>64.407005</td>\n      <td>39.586609</td>\n      <td>0.0</td>\n      <td>49.932187</td>\n      <td>39.67777</td>\n      <td>54.337107</td>\n      <td>9.386922</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AL</td>\n      <td>5</td>\n      <td>48.0</td>\n      <td>73.406937</td>\n      <td>48.77541</td>\n      <td>0.0</td>\n      <td>48.941521</td>\n      <td>48.669454</td>\n      <td>63.3096</td>\n      <td>8.659783</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>117031</th>\n      <td>NE</td>\n      <td>8</td>\n      <td>25.909684</td>\n      <td>85.4</td>\n      <td>62.7</td>\n      <td>-32.307496</td>\n      <td>40.020323</td>\n      <td>59.955251</td>\n      <td>74.535821</td>\n      <td>7.050787</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>117032</th>\n      <td>NE</td>\n      <td>9</td>\n      <td>46.810539</td>\n      <td>77.8</td>\n      <td>53.6</td>\n      <td>-337.711363</td>\n      <td>41.015647</td>\n      <td>53.890063</td>\n      <td>67.271141</td>\n      <td>7.402396</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>117033</th>\n      <td>NE</td>\n      <td>10</td>\n      <td>47.409751</td>\n      <td>65.2</td>\n      <td>40.6</td>\n      <td>-656.829311</td>\n      <td>45.018233</td>\n      <td>44.25134</td>\n      <td>56.376711</td>\n      <td>7.768246</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>117034</th>\n      <td>NE</td>\n      <td>11</td>\n      <td>49.541899</td>\n      <td>49.7</td>\n      <td>28.0</td>\n      <td>-363.569282</td>\n      <td>51.45818</td>\n      <td>35.445112</td>\n      <td>45.895488</td>\n      <td>8.266765</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>117035</th>\n      <td>NE</td>\n      <td>12</td>\n      <td>50.036534</td>\n      <td>36.9</td>\n      <td>17.1</td>\n      <td>-291.094614</td>\n      <td>55.725457</td>\n      <td>28.84527</td>\n      <td>37.656348</td>\n      <td>8.419217</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>117036 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adds another column showing which bin every row falls into. The bins are based on the temp_max_normalRay column\n",
    "df['longitude_bin'] = pd.cut(df1['longitude'],\n",
    "                             bins=[-400, -125, -100, -75, np.inf],\n",
    "                             labels=[1, 2, 3, 4])\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,\n",
    "                               random_state=42)  #n_splits is training date, test_size is target\n",
    "\n",
    "for train_index, test_index in split.split(df, df[\"longitude_bin\"]):  #shows it is an iterateable object\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]\n",
    "\n",
    "\n",
    "#SELECT COUNT(*) GROUP BY region;\n",
    "#     check if we need strat_sampling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "lab_train1 = strat_train_set['state']\n",
    "feat_train1 = strat_train_set.drop(['state', 'longitude_bin'], axis=1)\n",
    "lab_test = strat_test_set['state']\n",
    "feat_test = strat_test_set.drop(['state', 'longitude_bin'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "lab_train = lab_train1.copy()\n",
    "feat_train = feat_train1.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "109481    CA\n42532     MS\n81805     TX\n4756      AR\n94199     WV\n          ..\n69720     OR\n98813     WY\n7775      CA\n106341    TN\n61922     NC\nName: state, Length: 93628, dtype: object"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "'from sklearn.model_selection import train_test_split\\n\\nfeat_train, feat_test, lab_train, lab_test = train_test_split(featPrepped, labels, random_state=42)\\n# stratify by city for separating training and testing. stratified sampling'"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler = StandardScaler()  # makes 0 the average\n",
    "feat_train = scaler.fit_transform(feat_train)\n",
    "feat_train = scaler2.fit_transform(feat_train)\n",
    "feat_train += 1\n",
    "\n",
    "\"\"\"from sklearn.model_selection import train_test_split\n",
    "\n",
    "feat_train, feat_test, lab_train, lab_test = train_test_split(featPrepped, labels, random_state=42)\n",
    "# stratify by city for separating training and testing. stratified sampling\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef48f30a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afaa8d6e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mplN = MLPClassifier(batch_size='auto', warm_start=True, max_iter=400)\n",
    "mplS = MLPClassifier(batch_size='auto', warm_start=True, solver='sgd', max_iter=400, early_stopping=True)\n",
    "mplA = MLPClassifier(batch_size='auto', warm_start=True, solver='adam', max_iter=400, early_stopping=True)\n",
    "\n",
    "ensemble_clf = [mplN,mplS,mplA]\n",
    "\n",
    "mplNParam = {\n",
    "    'hidden_layer_sizes': (10,120,10),\n",
    "    'activation': ('identity', 'logistic', 'tanh', 'relu'),\n",
    "    'alpha': (0.000001, 0.00001, 0.0001),\n",
    "    'solver': ('lbfgs', 'sgd', 'adam'),\n",
    "    'shrinking': (True, False),\n",
    "}\n",
    "mplSParam = {\n",
    "    'hidden_layer_sizes': (10,120,10),\n",
    "    'activation': ('identity', 'logistic', 'tanh', 'relu'),\n",
    "    'alpha': (0.000001, 0.00001, 0.0001),\n",
    "    'learning_rate': ('constant', 'invscaling', 'adaptive'),\n",
    "    'momentum': (0.1,0.9,0.1),\n",
    "}\n",
    "mplAParam = {\n",
    "    'hidden_layer_sizes': (10,120,10),\n",
    "    'activation': ('identity', 'logistic', 'tanh', 'relu'),\n",
    "    'alpha': (0.000001, 0.00001, 0.0001),\n",
    "    'beta_1': (0.1,0.9,0.1),\n",
    "    'beta_2': (0.1,0.9,0.1),\n",
    "                   }\n",
    "\n",
    "parameters_list = [rfParam, exParamOob, exParam, bnbParam, cnbParam, gnbParam, rnParam, lrParam, rcParam, svcParam, lsvcParam,\n",
    "                   nuParam, mplNParam, mplSParam, mplAParam, ncParam, qdaParam, knParam, gpcParam, sgdParam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = dt.now()\n",
    "\n",
    "progress_bar = tqdm(total=len(ensemble_clf), desc='Reading models...')\n",
    "\n",
    "Grid = []\n",
    "\n",
    "for i in range(len(ensemble_clf)):\n",
    "    modelStart = dt.now()\n",
    "    try:\n",
    "        Grid = HalvingGridSearchCV(estimator=ensemble_clf[i], param_grid=parameters_list[i],\n",
    "                                   n_jobs=-1, cv=3, verbose=0).fit(feat_train, lab_train)\n",
    "        print(\"best param: \", Grid.best_params_)\n",
    "        print(\"best score: \", Grid.best_score_)\n",
    "        #base_accuracy = evaluate(ensemble_clf, feat_test, lab_test)\n",
    "        #grid_accuracy = evaluate(Grid.best_estimator, feat_test, lab_test)\n",
    "        #print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))\n",
    "\n",
    "    except:\n",
    "        print(\"model \", ensemble_clf[i], \"did not work\")\n",
    "    progress_bar.update(1)\n",
    "    modelRunning_secs = (dt.now() - modelStart).seconds\n",
    "    print(\"Model \", ensemble_clf[i], \" running time: \", modelRunning_secs)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "running_secs = (dt.now() - start).seconds\n",
    "print(\"Program running time: \", running_secs)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/07/performing-multi-class-classification-on-fifa-dataset-using-keras/\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam, Adadelta, RMSprop\n",
    "import keras.backend as K\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(60, input_shape = (33,), activation = \"relu\"))\n",
    "model.add(Dense(15, activation = \"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4, activation = \"softmax\"))\n",
    "model.compile(Adam(lr = 0.01), \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, verbose=1, epochs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_class = model.predict_classes(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(x_test)\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "confusion_matrix(y_test_class, y_pred_class)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_class, y_pred_class))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "\"\"\"https://www.kaggle.com/code/nitishkulkarni1006/multi-class-classification-with-keras-tensorflow/notebook\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "# Convert target Y to one hot encoded Y for Neural Network\n",
    "#lab_train = pd.get_dummies(lab_train)\n",
    "# If target is in string form, use following code:\n",
    "# First encode target values as integers from string\n",
    "# Then perform one hot encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(lab_train)\n",
    "lab_train = encoder.transform(lab_train)\n",
    "feat_train = np_utils.to_categorical(feat_train, 60)\n",
    "lab_train = np_utils.to_categorical(lab_train, 60)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [95]\u001B[0m, in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mno\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m lab_train \u001B[38;5;241m=\u001B[39m \u001B[43mlab_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "# For Keras, convert dataframe to array values (Inbuilt requirement of Keras)\n",
    "try:\n",
    "    feat_train = feat_train.values\n",
    "except:\n",
    "    print(\"no\")\n",
    "lab_train = lab_train.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "'Two hidden layers are defined with \"Rectified Linear Unit\" (relu) and 15 neurons each. Furthermore, this is a multi-class classification problem and there are total 11 target clsses, therefore \"softmax\" activation function and 11 neurons are used in the output layer. For hidden layers, the number of neurons should be in between the input data dimension and the output data dimension. In this case, the input data has 48 variable columns and output classes are 11. Therefore, the number of neurons for the hidden layer should be in between 11 and 48. You can try different values for the number of neurons as well as different number of hidden layers.\\nInput dimension (input_dim) is 48, because the input variable columns are 48. It will change as per the dimension of the input variables.\\n\\n'"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Two hidden layers are defined with \"Rectified Linear Unit\" (relu) and 15 neurons each. Furthermore, this is a multi-class classification problem and there are total 11 target clsses, therefore \"softmax\" activation function and 11 neurons are used in the output layer. For hidden layers, the number of neurons should be in between the input data dimension and the output data dimension. In this case, the input data has 48 variable columns and output classes are 11. Therefore, the number of neurons for the hidden layer should be in between 11 and 48. You can try different values for the number of neurons as well as different number of hidden layers.\n",
    "Input dimension (input_dim) is 48, because the input variable columns are 48. It will change as per the dimension of the input variables.\n",
    "\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "# First define baseline model. Then use it in Keras Classifier for the training\n",
    "def baseline_model():\n",
    "    # Create model here\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim = 9, activation = 'relu')) # Rectified Linear Unit Activation Function\n",
    "    model.add(Dense(15, activation = 'relu'))\n",
    "    model.add(Dense(60, activation = 'softmax')) # Softmax for multi-class classification\n",
    "    # Compile model here\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trent\\AppData\\Local\\Temp\\ipykernel_10740\\2645294572.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  estimator = KerasClassifier(build_fn = baseline_model, epochs = 100, batch_size = 10, verbose = 0)\n"
     ]
    }
   ],
   "source": [
    "# Create Keras Classifier and use predefined baseline model\n",
    "estimator = KerasClassifier(build_fn = baseline_model, epochs = 100, batch_size = 10, verbose = 0)\n",
    "# Try different values for epoch and batch size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "# KFold Cross Validation\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "# Try different values of splits e.g., 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 9) for input KerasTensor(type_spec=TensorSpec(shape=(None, 9), dtype=tf.float32, name='dense_24_input'), name='dense_24_input', description=\"created by layer 'dense_24_input'\"), but it was called on an input with incompatible shape (None, 9, 60).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Trent\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Trent\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Trent\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Trent\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Trent\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Trent\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 248, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_8\" (type Sequential).\n    \n    Input 0 of layer \"dense_24\" is incompatible with the layer: expected axis -1 of input shape to have value 9, but received input with shape (None, 9, 60)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 9, 60), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [100]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Object to describe the result\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mcross_val_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeat_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlab_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mkfold\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mraise\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Result\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mResult: \u001B[39m\u001B[38;5;132;01m%.2f\u001B[39;00m\u001B[38;5;132;01m%%\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m%.2f\u001B[39;00m\u001B[38;5;132;01m%%\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (results\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m, results\u001B[38;5;241m.\u001B[39mstd()\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m100\u001B[39m))\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:509\u001B[0m, in \u001B[0;36mcross_val_score\u001B[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001B[0m\n\u001B[0;32m    506\u001B[0m \u001B[38;5;66;03m# To ensure multimetric format is not supported\u001B[39;00m\n\u001B[0;32m    507\u001B[0m scorer \u001B[38;5;241m=\u001B[39m check_scoring(estimator, scoring\u001B[38;5;241m=\u001B[39mscoring)\n\u001B[1;32m--> 509\u001B[0m cv_results \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    510\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    511\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    512\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    513\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    514\u001B[0m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mscore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    515\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    516\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    517\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    518\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    519\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    520\u001B[0m \u001B[43m    \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    521\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cv_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_score\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:267\u001B[0m, in \u001B[0;36mcross_validate\u001B[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001B[0m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# independent, and that it is pickle-able.\u001B[39;00m\n\u001B[0;32m    266\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[1;32m--> 267\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscorers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    276\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    277\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    278\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    279\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_times\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    280\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_estimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_estimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    281\u001B[0m \u001B[43m        \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    282\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    283\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    284\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    286\u001B[0m _warn_about_fit_failures(results, error_score)\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[0;32m    289\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[0;32m    290\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\joblib\\parallel.py:1043\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1034\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1035\u001B[0m     \u001B[38;5;66;03m# Only set self._iterating to True if at least a batch\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m     \u001B[38;5;66;03m# was dispatched. In particular this covers the edge\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1040\u001B[0m     \u001B[38;5;66;03m# was very quick and its callback already dispatched all the\u001B[39;00m\n\u001B[0;32m   1041\u001B[0m     \u001B[38;5;66;03m# remaining jobs.\u001B[39;00m\n\u001B[0;32m   1042\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m-> 1043\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdispatch_one_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1044\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1046\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\joblib\\parallel.py:861\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[1;34m(self, iterator)\u001B[0m\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    860\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 861\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dispatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\joblib\\parallel.py:779\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m    778\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[1;32m--> 779\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backend\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    780\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[0;32m    781\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[0;32m    782\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[1;34m(self, func, callback)\u001B[0m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mImmediateResult\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[0;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[1;32m--> 572\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m \u001B[43mbatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\joblib\\parallel.py:262\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    258\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[0;32m    261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[1;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    263\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig):\n\u001B[1;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[0;32m    678\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    679\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 680\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    682\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    683\u001B[0m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[0;32m    684\u001B[0m     fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py:236\u001B[0m, in \u001B[0;36mKerasClassifier.fit\u001B[1;34m(self, x, y, **kwargs)\u001B[0m\n\u001B[0;32m    234\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInvalid shape for y: \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(y\u001B[38;5;241m.\u001B[39mshape))\n\u001B[0;32m    235\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_classes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclasses_)\n\u001B[1;32m--> 236\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(KerasClassifier, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mfit(x, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py:164\u001B[0m, in \u001B[0;36mBaseWrapper.fit\u001B[1;34m(self, x, y, **kwargs)\u001B[0m\n\u001B[0;32m    161\u001B[0m fit_args \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfilter_sk_params(Sequential\u001B[38;5;241m.\u001B[39mfit))\n\u001B[0;32m    162\u001B[0m fit_args\u001B[38;5;241m.\u001B[39mupdate(kwargs)\n\u001B[1;32m--> 164\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mfit(x, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_args)\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m history\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=broad-except\u001B[39;00m\n\u001B[0;32m     66\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m---> 67\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\Documents\\ml-team3\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint:disable=broad-except\u001B[39;00m\n\u001B[0;32m   1146\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(e, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mag_error_metadata\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1147\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mag_error_metadata\u001B[38;5;241m.\u001B[39mto_exception(e)\n\u001B[0;32m   1148\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1149\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: in user code:\n\n    File \"C:\\Users\\Trent\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Trent\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Trent\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Trent\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Trent\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Trent\\Documents\\ml-team3\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 248, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential_8\" (type Sequential).\n    \n    Input 0 of layer \"dense_24\" is incompatible with the layer: expected axis -1 of input shape to have value 9, but received input with shape (None, 9, 60)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 9, 60), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "# Object to describe the result\n",
    "results = cross_val_score(estimator, feat_train, lab_train, cv = kfold, error_score='raise')\n",
    "# Result\n",
    "print(\"Result: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.]],\n\n       [[0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.]],\n\n       [[0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.]],\n\n       ...,\n\n       [[0., 0., 1., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.]],\n\n       [[0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.]],\n\n       [[0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.],\n        [0., 1., 0., ..., 0., 0., 0.]]], dtype=float32)"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "\"\"\"\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"iris.data\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"https://sites.google.com/site/nttrungmtwiki/home/it/data-science---python/multi-class-classification-tutorial-with-the-keras-deep-learning-library\n",
    "\"\"\"\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    " # create model\n",
    " model = Sequential()\n",
    " model.add(Dense(4, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
    " model.add(Dense(3, kernel_initializer='normal', activation='sigmoid'))\n",
    " # Compile model\n",
    " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}