{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d16ee7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules from repository\n",
    "from model.data.pipeline import utils as ut\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3031ba1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe of all monthly data\n",
    "df = ut.load_sql_as_df('SELECT * From MonthlyDataModel;')\n",
    "\n",
    "#dataframe of month averages to fill in bad values\n",
    "monthAvg = ut.load_sql_as_df('SELECT * From MonthlyAverages;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Getting features and choosing labels. Fill NaN spots with NaN text to manipulate them\n",
    "labels = df[\"state\"]\n",
    "#['region', 'latitude', 'longitude', 'elevation', 'region'] are all dropped automatically\n",
    "df = df.fillna('NaN')\n",
    "\n",
    "# Changing columns in to months to help compare\n",
    "featSer = df[\"month\"].to_numpy()\n",
    "prcpIntM = monthAvg[\"prcpInt\"].to_numpy()\n",
    "prcpFreqM = monthAvg[\"prcpFreq\"].to_numpy()\n",
    "temp_max_normalM = monthAvg[\"temp_max_normal\"].to_numpy()\n",
    "temp_min_normalM = monthAvg[\"temp_min_normal\"].to_numpy()\n",
    "snow_depthM = monthAvg[\"snowInt\"].to_numpy()\n",
    "snow_daysM = monthAvg[\"snowFreq\"].to_numpy()\n",
    "cloudsM = monthAvg[\"clouds\"].to_numpy()\n",
    "dewM = monthAvg[\"dew_point\"].to_numpy()\n",
    "heatM = monthAvg[\"heat_index\"].to_numpy()\n",
    "pressureM = monthAvg[\"pressure\"].to_numpy()\n",
    "windM = monthAvg[\"wind_speed\"].to_numpy()\n",
    "windCalmM = monthAvg[\"wind_calm_percentage\"].to_numpy()\n",
    "\n",
    "prcp_normRay = df[\"prcp_normal\"].to_numpy()\n",
    "prcp_days_tRay = df[\"prcp_days_t\"].to_numpy()\n",
    "temp_max_normalRay = df[\"temp_max_normal\"].to_numpy()\n",
    "temp_min_normalRay = df[\"temp_min_normal\"].to_numpy()\n",
    "snow_depthRay = df[\"snow_depth_days\"].to_numpy()\n",
    "snow_daysRay = df[\"snow_days_t\"].to_numpy()\n",
    "cloudsRay = df[\"clouds\"].to_numpy()\n",
    "dewRay = df[\"dew_point\"].to_numpy()\n",
    "heatRay = df[\"heat_index\"].to_numpy()\n",
    "pressureRay = df[\"pressure\"].to_numpy()\n",
    "windRay = df[\"wind_speed\"].to_numpy()\n",
    "windCalmRay = df[\"wind_calm_percentage\"].to_numpy()\n",
    "\n",
    "#replacing bad values\n",
    "for i in range(len(featSer)):\n",
    "    if prcp_normRay[i] == 'NaN' or prcp_normRay[i] == -9999 or prcp_normRay[i] == -8888 or prcp_normRay[i] == -7777 or prcp_normRay[i] == -6666 or prcp_normRay[i] == -5555:\n",
    "        prcp_normRay[i] = prcpIntM[featSer[i] - 1]\n",
    "    if prcp_days_tRay[i] == 'NaN' or prcp_days_tRay[i] == -9999 or prcp_days_tRay[i] == -8888 or prcp_days_tRay[\n",
    "        i] == -7777 or prcp_days_tRay[i] == -6666 or prcp_days_tRay[i] == -5555:\n",
    "        prcp_days_tRay[i] = prcpFreqM[featSer[i] - 1]\n",
    "    if temp_max_normalRay[i] == 'NaN' or temp_max_normalRay[i] == -9999 or temp_max_normalRay[i] == -8888 or temp_max_normalRay[i] == -7777 or temp_max_normalRay[i] == -6666 or temp_max_normalRay[i] == -5555:\n",
    "        temp_max_normalRay[i] = temp_max_normalM[featSer[i] - 1]\n",
    "    if temp_min_normalRay[i] == 'NaN' or temp_min_normalRay[i] == -9999 or temp_min_normalRay[i] == -8888 or temp_min_normalRay[i] == -7777 or temp_min_normalRay[i] == -6666 or temp_min_normalRay[i] == -5555:\n",
    "        temp_min_normalRay[i] = temp_min_normalM[featSer[i] - 1]\n",
    "    if snow_depthRay[i] == 'NaN' or snow_depthRay[i] == -9999 or snow_depthRay[i] == -8888 or snow_depthRay[i] == -7777 or snow_depthRay[i] == -6666 or snow_depthRay[i] == -5555:\n",
    "        snow_depthRay[i] = snow_depthM[featSer[i] - 1]\n",
    "    if snow_daysRay[i] == 'NaN' or snow_daysRay[i] == -9999 or snow_daysRay[i] == -8888 or snow_daysRay[i] == -7777 or snow_daysRay[i] ==-6666 or snow_daysRay[i] == -5555:\n",
    "        snow_daysRay[i] = snow_daysM[featSer[i] - 1]\n",
    "    if cloudsRay[i] == 'NaN' or cloudsRay[i] == -9999 or cloudsRay[i] == -8888 or cloudsRay[i] == -7777 or cloudsRay[i] == -6666 or cloudsRay[i] == -5555:\n",
    "        cloudsRay[i] = cloudsM[featSer[i] - 1]\n",
    "    if dewRay[i] == 'NaN' or dewRay[i] == -9999 or dewRay[i] == -8888 or dewRay[i] == -7777 or dewRay[i] == -6666 or dewRay[i] == -5555:\n",
    "        dewRay[i] = dewM[featSer[i] - 1]\n",
    "    if heatRay[i] == 'NaN' or heatRay[i] == -9999 or heatRay[i] == -8888 or heatRay[i] == -7777 or heatRay[i] == -6666 or heatRay[i] == -5555:\n",
    "        heatRay[i] = heatM[featSer[i] - 1]\n",
    "    if pressureRay[i] == 'NaN' or pressureRay[i] == -9999 or pressureRay[i] == -8888 or pressureRay[i] == -7777 or pressureRay[i] == -6666 or pressureRay[i] == -5555:\n",
    "        pressureRay[i] = pressureM[featSer[i] - 1]\n",
    "    if windRay[i] == 'NaN' or windRay[i] == -9999 or windRay[i] == -8888 or windRay[i] == -7777 or windRay[i] == -6666 or windRay[i] == -5555:\n",
    "        windRay[i] = windM[featSer[i] - 1]\n",
    "    if windCalmRay[i] == 'NaN' or windCalmRay[i] == -9999 or windCalmRay[i] == -8888 or windCalmRay[i] == -7777 or windCalmRay[i] == -6666 or windCalmRay[i] == -5555:\n",
    "        windCalmRay[i] = windCalmM[featSer[i] - 1]\n",
    "\n",
    "df1 = df.copy()\n",
    "# Recreating filled dataset\n",
    "df = pd.DataFrame({'state': labels, 'prcp_normRay': prcp_normRay, 'prcp_days_tRay': prcp_days_tRay,\n",
    "                   \"temp_max_normalRay\": temp_max_normalRay, \"temp_min_normalRay\": temp_min_normalRay,\n",
    "                   \"snow_depthRay\": snow_depthRay, \"snow_daysRay\": snow_daysRay, \"cloudsRay\": cloudsRay,\n",
    "                   \"dewRay\": dewRay, \"heatRay\": heatRay, \"pressureRay\": pressureRay, \"windRay\": windRay,\n",
    "                   \"windCalmRay\": windCalmRay})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "       state prcp_normRay prcp_days_tRay temp_max_normalRay  \\\n0         AL          5.3           69.0          41.924644   \n1         AL         5.07           65.0          45.985372   \n2         AL          5.8           62.0          54.495133   \n3         AL         3.77           48.0          64.407005   \n4         AL         3.82           48.0          73.406937   \n...      ...          ...            ...                ...   \n117031    NE         3.22      25.909684               85.4   \n117032    NE         3.04      46.810539               77.8   \n117033    NE         2.04      47.409751               65.2   \n117034    NE         1.41      49.541899               49.7   \n117035    NE         0.93      50.036534               36.9   \n\n       temp_min_normalRay snow_depthRay snow_daysRay  cloudsRay     dewRay  \\\n0               21.303576           0.0          0.0  55.667977  26.341895   \n1               24.106816           0.0          0.0  55.013862  28.228661   \n2               31.252366    -309.95931  -297.336583  52.051446  33.052022   \n3               39.586609           0.0          0.0  49.932187   39.67777   \n4                48.77541           0.0          0.0  48.941521  48.669454   \n...                   ...           ...          ...        ...        ...   \n117031               62.7     -6.026158   -32.307496  40.020323  59.955251   \n117032               53.6   -156.429374  -337.711363  41.015647  53.890063   \n117033               40.6   -553.695795  -656.829311  45.018233   44.25134   \n117034               28.0   -244.419299  -363.569282   51.45818  35.445112   \n117035               17.1   -238.601821  -291.094614  55.725457   28.84527   \n\n          heatRay  pressureRay   windRay windCalmRay longitude_bin  \n0       35.417982  1018.967064  8.593669    14.68457             3  \n1       38.559721  1018.084567  8.813964   13.644477             3  \n2       45.700434   1016.38444  9.329369   11.746978             3  \n3       54.337107  1014.836497  9.386922    11.09783             3  \n4         63.3096   1014.35943  8.659783   12.057727             3  \n...           ...          ...       ...         ...           ...  \n117031  74.535821   1015.39399  7.050787   16.593875             3  \n117032  67.271141  1015.676289  7.402396   16.429875             3  \n117033  56.376711   1016.88648  7.768246   16.520839             3  \n117034  45.895488  1017.744118  8.266765   15.842452             3  \n117035  37.656348  1018.840951  8.419217   15.283866             3  \n\n[117036 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>state</th>\n      <th>prcp_normRay</th>\n      <th>prcp_days_tRay</th>\n      <th>temp_max_normalRay</th>\n      <th>temp_min_normalRay</th>\n      <th>snow_depthRay</th>\n      <th>snow_daysRay</th>\n      <th>cloudsRay</th>\n      <th>dewRay</th>\n      <th>heatRay</th>\n      <th>pressureRay</th>\n      <th>windRay</th>\n      <th>windCalmRay</th>\n      <th>longitude_bin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AL</td>\n      <td>5.3</td>\n      <td>69.0</td>\n      <td>41.924644</td>\n      <td>21.303576</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>55.667977</td>\n      <td>26.341895</td>\n      <td>35.417982</td>\n      <td>1018.967064</td>\n      <td>8.593669</td>\n      <td>14.68457</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AL</td>\n      <td>5.07</td>\n      <td>65.0</td>\n      <td>45.985372</td>\n      <td>24.106816</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>55.013862</td>\n      <td>28.228661</td>\n      <td>38.559721</td>\n      <td>1018.084567</td>\n      <td>8.813964</td>\n      <td>13.644477</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AL</td>\n      <td>5.8</td>\n      <td>62.0</td>\n      <td>54.495133</td>\n      <td>31.252366</td>\n      <td>-309.95931</td>\n      <td>-297.336583</td>\n      <td>52.051446</td>\n      <td>33.052022</td>\n      <td>45.700434</td>\n      <td>1016.38444</td>\n      <td>9.329369</td>\n      <td>11.746978</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AL</td>\n      <td>3.77</td>\n      <td>48.0</td>\n      <td>64.407005</td>\n      <td>39.586609</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>49.932187</td>\n      <td>39.67777</td>\n      <td>54.337107</td>\n      <td>1014.836497</td>\n      <td>9.386922</td>\n      <td>11.09783</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AL</td>\n      <td>3.82</td>\n      <td>48.0</td>\n      <td>73.406937</td>\n      <td>48.77541</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>48.941521</td>\n      <td>48.669454</td>\n      <td>63.3096</td>\n      <td>1014.35943</td>\n      <td>8.659783</td>\n      <td>12.057727</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>117031</th>\n      <td>NE</td>\n      <td>3.22</td>\n      <td>25.909684</td>\n      <td>85.4</td>\n      <td>62.7</td>\n      <td>-6.026158</td>\n      <td>-32.307496</td>\n      <td>40.020323</td>\n      <td>59.955251</td>\n      <td>74.535821</td>\n      <td>1015.39399</td>\n      <td>7.050787</td>\n      <td>16.593875</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>117032</th>\n      <td>NE</td>\n      <td>3.04</td>\n      <td>46.810539</td>\n      <td>77.8</td>\n      <td>53.6</td>\n      <td>-156.429374</td>\n      <td>-337.711363</td>\n      <td>41.015647</td>\n      <td>53.890063</td>\n      <td>67.271141</td>\n      <td>1015.676289</td>\n      <td>7.402396</td>\n      <td>16.429875</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>117033</th>\n      <td>NE</td>\n      <td>2.04</td>\n      <td>47.409751</td>\n      <td>65.2</td>\n      <td>40.6</td>\n      <td>-553.695795</td>\n      <td>-656.829311</td>\n      <td>45.018233</td>\n      <td>44.25134</td>\n      <td>56.376711</td>\n      <td>1016.88648</td>\n      <td>7.768246</td>\n      <td>16.520839</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>117034</th>\n      <td>NE</td>\n      <td>1.41</td>\n      <td>49.541899</td>\n      <td>49.7</td>\n      <td>28.0</td>\n      <td>-244.419299</td>\n      <td>-363.569282</td>\n      <td>51.45818</td>\n      <td>35.445112</td>\n      <td>45.895488</td>\n      <td>1017.744118</td>\n      <td>8.266765</td>\n      <td>15.842452</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>117035</th>\n      <td>NE</td>\n      <td>0.93</td>\n      <td>50.036534</td>\n      <td>36.9</td>\n      <td>17.1</td>\n      <td>-238.601821</td>\n      <td>-291.094614</td>\n      <td>55.725457</td>\n      <td>28.84527</td>\n      <td>37.656348</td>\n      <td>1018.840951</td>\n      <td>8.419217</td>\n      <td>15.283866</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>117036 rows × 14 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adds another column showing which bin every row falls into. The bins are based on the temp_max_normalRay column\n",
    "df['longitude_bin'] = pd.cut(df1['longitude'],\n",
    "                             bins=[-400, -125, -100, -75, np.inf],\n",
    "                             labels=[1, 2, 3, 4])\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,\n",
    "                               random_state=42)  #n_splits is training date, test_size is target\n",
    "\n",
    "for train_index, test_index in split.split(df, df[\"longitude_bin\"]):  #shows it is an iterateable object\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]\n",
    "\n",
    "\n",
    "#SELECT COUNT(*) GROUP BY region;\n",
    "#     check if we need strat_sampling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "lab_train = strat_train_set['state']\n",
    "feat_train = strat_train_set.drop(['state', 'longitude_bin'], axis=1)\n",
    "lab_test = strat_test_set['state']\n",
    "feat_test = strat_test_set.drop(['state', 'longitude_bin'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'from sklearn.model_selection import train_test_split\\n\\nfeat_train, feat_test, lab_train, lab_test = train_test_split(featPrepped, labels, random_state=42)\\n# stratify by city for separating training and testing. stratified sampling'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler = StandardScaler()  # makes 0 the average\n",
    "feat_train = scaler.fit_transform(feat_train)\n",
    "feat_train = scaler2.fit_transform(feat_train)\n",
    "feat_train += 1\n",
    "\n",
    "\"\"\"from sklearn.model_selection import train_test_split\n",
    "\n",
    "feat_train, feat_test, lab_train, lab_test = train_test_split(featPrepped, labels, random_state=42)\n",
    "# stratify by city for separating training and testing. stratified sampling\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef48f30a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afaa8d6e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mplN = MLPClassifier(batch_size='auto', warm_start=True, max_iter=400)\n",
    "mplS = MLPClassifier(batch_size='auto', warm_start=True, solver='sgd', max_iter=400, early_stopping=True)\n",
    "mplA = MLPClassifier(batch_size='auto', warm_start=True, solver='adam', max_iter=400, early_stopping=True)\n",
    "\n",
    "ensemble_clf = [mplN,mplS,mplA]\n",
    "\n",
    "mplNParam = {\n",
    "    'hidden_layer_sizes': (10,120,10),\n",
    "    'activation': ('identity', 'logistic', 'tanh', 'relu'),\n",
    "    'alpha': (0.000001, 0.00001, 0.0001),\n",
    "    'solver': ('lbfgs', 'sgd', 'adam'),\n",
    "    'shrinking': (True, False),\n",
    "}\n",
    "mplSParam = {\n",
    "    'hidden_layer_sizes': (10,120,10),\n",
    "    'activation': ('identity', 'logistic', 'tanh', 'relu'),\n",
    "    'alpha': (0.000001, 0.00001, 0.0001),\n",
    "    'learning_rate': ('constant', 'invscaling', 'adaptive'),\n",
    "    'momentum': (0.1,0.9,0.1),\n",
    "}\n",
    "mplAParam = {\n",
    "    'hidden_layer_sizes': (10,120,10),\n",
    "    'activation': ('identity', 'logistic', 'tanh', 'relu'),\n",
    "    'alpha': (0.000001, 0.00001, 0.0001),\n",
    "    'beta_1': (0.1,0.9,0.1),\n",
    "    'beta_2': (0.1,0.9,0.1),\n",
    "                   }\n",
    "\n",
    "parameters_list = [rfParam, exParamOob, exParam, bnbParam, cnbParam, gnbParam, rnParam, lrParam, rcParam, svcParam, lsvcParam,\n",
    "                   nuParam, mplNParam, mplSParam, mplAParam, ncParam, qdaParam, knParam, gpcParam, sgdParam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start = dt.now()\n",
    "\n",
    "progress_bar = tqdm(total=len(ensemble_clf), desc='Reading models...')\n",
    "\n",
    "Grid = []\n",
    "\n",
    "for i in range(len(ensemble_clf)):\n",
    "    modelStart = dt.now()\n",
    "    try:\n",
    "        Grid = HalvingGridSearchCV(estimator=ensemble_clf[i], param_grid=parameters_list[i],\n",
    "                                   n_jobs=-1, cv=3, verbose=0).fit(feat_train, lab_train)\n",
    "        print(\"best param: \", Grid.best_params_)\n",
    "        print(\"best score: \", Grid.best_score_)\n",
    "        #base_accuracy = evaluate(ensemble_clf, feat_test, lab_test)\n",
    "        #grid_accuracy = evaluate(Grid.best_estimator, feat_test, lab_test)\n",
    "        #print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))\n",
    "\n",
    "    except:\n",
    "        print(\"model \", ensemble_clf[i], \"did not work\")\n",
    "    progress_bar.update(1)\n",
    "    modelRunning_secs = (dt.now() - modelStart).seconds\n",
    "    print(\"Model \", ensemble_clf[i], \" running time: \", modelRunning_secs)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "running_secs = (dt.now() - start).seconds\n",
    "print(\"Program running time: \", running_secs)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "https://www.analyticsvidhya.com/blog/2021/07/performing-multi-class-classification-on-fifa-dataset-using-keras/\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam, Adadelta, RMSprop\n",
    "import keras.backend as K\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(60, input_shape = (33,), activation = \"relu\"))\n",
    "model.add(Dense(15, activation = \"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4, activation = \"softmax\"))\n",
    "model.compile(Adam(lr = 0.01), \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, verbose=1, epochs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_class = model.predict_classes(x_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(x_test)\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "confusion_matrix(y_test_class, y_pred_class)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test_class, y_pred_class))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"https://www.kaggle.com/code/nitishkulkarni1006/multi-class-classification-with-keras-tensorflow/notebook\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from keras.utils import np_utils\n",
    "#from sklearn.preprocessing import LabelEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert target Y to one hot encoded Y for Neural Network\n",
    "Y = pd.get_dummies(Y)\n",
    "# If target is in string form, use following code:\n",
    "# First encode target values as integers from string\n",
    "# Then perform one hot encoding\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(Y)\n",
    "# Y = encoder.transform(Y)\n",
    "# Y = np_utils.to_categorical(Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# For Keras, convert dataframe to array values (Inbuilt requirement of Keras)\n",
    "X = X.values\n",
    "Y = Y.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"Two hidden layers are defined with \"Rectified Linear Unit\" (relu) and 15 neurons each. Furthermore, this is a multi-class classification problem and there are total 11 target clsses, therefore \"softmax\" activation function and 11 neurons are used in the output layer. For hidden layers, the number of neurons should be in between the input data dimension and the output data dimension. In this case, the input data has 48 variable columns and output classes are 11. Therefore, the number of neurons for the hidden layer should be in between 11 and 48. You can try different values for the number of neurons as well as different number of hidden layers.\n",
    "Input dimension (input_dim) is 48, because the input variable columns are 48. It will change as per the dimension of the input variables.\n",
    "\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# First define baseline model. Then use it in Keras Classifier for the training\n",
    "def baseline_model():\n",
    "    # Create model here\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15, input_dim = 48, activation = 'relu')) # Rectified Linear Unit Activation Function\n",
    "    model.add(Dense(15, activation = 'relu'))\n",
    "    model.add(Dense(11, activation = 'softmax')) # Softmax for multi-class classification\n",
    "    # Compile model here\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create Keras Classifier and use predefined baseline model\n",
    "estimator = KerasClassifier(build_fn = baseline_model, epochs = 100, batch_size = 10, verbose = 0)\n",
    "# Try different values for epoch and batch size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# KFold Cross Validation\n",
    "kfold = KFold(n_splits = 5, shuffle = True, random_state = seed)\n",
    "# Try different values of splits e.g., 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Object to describe the result\n",
    "results = cross_val_score(estimator, X, Y, cv = kfold)\n",
    "# Result\n",
    "print(\"Result: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "\"\"\"\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"iris.data\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"https://sites.google.com/site/nttrungmtwiki/home/it/data-science---python/multi-class-classification-tutorial-with-the-keras-deep-learning-library\n",
    "\"\"\"\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    " # create model\n",
    " model = Sequential()\n",
    " model.add(Dense(4, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
    " model.add(Dense(3, kernel_initializer='normal', activation='sigmoid'))\n",
    " # Compile model\n",
    " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}