{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d16ee7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Import necessary modules from repository\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodel\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipeline\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils \u001B[38;5;28;01mas\u001B[39;00m ut\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
      "File \u001B[1;32m~\\AI\\ml-team3\\model\\data\\pipeline\\utils.py:5\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mThese are utility methods for the pipeline package, particularly the\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03maggregate.py file.\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodel\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m db\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_sql_as_df\u001B[39m(query: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame:\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import necessary modules from repository\n",
    "from model.data.pipeline import utils as ut\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3031ba1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#dataframe of all monthly data\n",
    "df = ut.load_sql_as_df('SELECT * From MonthlyDataModel;')\n",
    "\n",
    "#dataframe of month averages to fill in bad values\n",
    "monthAvg = ut.load_sql_as_df('SELECT * From MonthlyAverages;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Getting features and choosing labels. Fill NaN spots with NaN text to manipulate them\n",
    "labels = df[\"state\"]\n",
    "#['region', 'latitude', 'longitude', 'elevation', 'region'] are all dropped automatically\n",
    "df = df.fillna('NaN')\n",
    "\n",
    "stateNump = df[\"state\"].to_numpy()\n",
    "\n",
    "# Changing columns in to months to help compare\n",
    "featSer = df[\"month\"].to_numpy()\n",
    "prcpIntM = monthAvg[\"prcpInt\"].to_numpy()\n",
    "prcpFreqM = monthAvg[\"prcpFreq\"].to_numpy()\n",
    "temp_max_normalM = monthAvg[\"temp_max_normal\"].to_numpy()\n",
    "temp_min_normalM = monthAvg[\"temp_min_normal\"].to_numpy()\n",
    "snow_depthM = monthAvg[\"snowInt\"].to_numpy()\n",
    "snow_daysM = monthAvg[\"snowFreq\"].to_numpy()\n",
    "cloudsM = monthAvg[\"clouds\"].to_numpy()\n",
    "dewM = monthAvg[\"dew_point\"].to_numpy()\n",
    "heatM = monthAvg[\"heat_index\"].to_numpy()\n",
    "pressureM = monthAvg[\"pressure\"].to_numpy()\n",
    "windM = monthAvg[\"wind_speed\"].to_numpy()\n",
    "windCalmM = monthAvg[\"wind_calm_percentage\"].to_numpy()\n",
    "\n",
    "prcp_normRay = df[\"prcp_normal\"].to_numpy()\n",
    "prcp_days_tRay = df[\"prcp_days_t\"].to_numpy()\n",
    "temp_max_normalRay = df[\"temp_max_normal\"].to_numpy()\n",
    "temp_min_normalRay = df[\"temp_min_normal\"].to_numpy()\n",
    "snow_depthRay = df[\"snow_depth_days\"].to_numpy()\n",
    "snow_daysRay = df[\"snow_days_t\"].to_numpy()\n",
    "cloudsRay = df[\"clouds\"].to_numpy()\n",
    "dewRay = df[\"dew_point\"].to_numpy()\n",
    "heatRay = df[\"heat_index\"].to_numpy()\n",
    "pressureRay = df[\"pressure\"].to_numpy()\n",
    "windRay = df[\"wind_speed\"].to_numpy()\n",
    "windCalmRay = df[\"wind_calm_percentage\"].to_numpy()\n",
    "\n",
    "#replacing bad values\n",
    "for i in range(len(featSer)):\n",
    "    if prcp_days_tRay[i] == 'NaN' or prcp_days_tRay[i] == -9999 or prcp_days_tRay[i] == -8888 or prcp_days_tRay[\n",
    "        i] == -7777 or prcp_days_tRay[i] == -6666 or prcp_days_tRay[i] == -5555:\n",
    "        prcp_days_tRay[i] = prcpFreqM[featSer[i] - 1]\n",
    "    if temp_max_normalRay[i] == 'NaN' or temp_max_normalRay[i] == -9999 or temp_max_normalRay[i] == -8888 or temp_max_normalRay[i] == -7777 or temp_max_normalRay[i] == -6666 or temp_max_normalRay[i] == -5555:\n",
    "        temp_max_normalRay[i] = temp_max_normalM[featSer[i] - 1]\n",
    "    if temp_min_normalRay[i] == 'NaN' or temp_min_normalRay[i] == -9999 or temp_min_normalRay[i] == -8888 or temp_min_normalRay[i] == -7777 or temp_min_normalRay[i] == -6666 or temp_min_normalRay[i] == -5555:\n",
    "        temp_min_normalRay[i] = temp_min_normalM[featSer[i] - 1]\n",
    "    if snow_daysRay[i] == 'NaN' or snow_daysRay[i] == -9999 or snow_daysRay[i] == -8888 or snow_daysRay[i] == -7777 or snow_daysRay[i] ==-6666 or snow_daysRay[i] == -5555:\n",
    "        snow_daysRay[i] = snow_daysM[featSer[i] - 1]\n",
    "    if cloudsRay[i] == 'NaN' or cloudsRay[i] == -9999 or cloudsRay[i] == -8888 or cloudsRay[i] == -7777 or cloudsRay[i] == -6666 or cloudsRay[i] == -5555:\n",
    "        cloudsRay[i] = cloudsM[featSer[i] - 1]\n",
    "    if dewRay[i] == 'NaN' or dewRay[i] == -9999 or dewRay[i] == -8888 or dewRay[i] == -7777 or dewRay[i] == -6666 or dewRay[i] == -5555:\n",
    "        dewRay[i] = dewM[featSer[i] - 1]\n",
    "    if heatRay[i] == 'NaN' or heatRay[i] == -9999 or heatRay[i] == -8888 or heatRay[i] == -7777 or heatRay[i] == -6666 or heatRay[i] == -5555:\n",
    "        heatRay[i] = heatM[featSer[i] - 1]\n",
    "    if windRay[i] == 'NaN' or windRay[i] == -9999 or windRay[i] == -8888 or windRay[i] == -7777 or windRay[i] == -6666 or windRay[i] == -5555:\n",
    "        windRay[i] = windM[featSer[i] - 1]\n",
    "\n",
    "mount = stateNump\n",
    "ocean = stateNump\n",
    "\n",
    "mountSwitcher = {\n",
    "'AL':1,\n",
    "'AR':2,\n",
    "'AZ':0,\n",
    "'CA':0,\n",
    "'CO':2,\n",
    "'CT':0,\n",
    "'DE':0,\n",
    "'FL':0,\n",
    "'GA':1,\n",
    "'HI':0,\n",
    "'IA':2,\n",
    "'ID':2,\n",
    "'IL':2,\n",
    "'IN':2,\n",
    "'KS':2,\n",
    "'KY':2,\n",
    "'LA':0,\n",
    "'MA':0,\n",
    "'MD':0,\n",
    "'ME':0,\n",
    "'MI':1,\n",
    "'MN':1,\n",
    "'MO':2,\n",
    "'MS':1,\n",
    "'MT':2,\n",
    "'NC':0,\n",
    "'ND':2,\n",
    "'NE':2,\n",
    "'NH':1,\n",
    "'NJ':0,\n",
    "'NM':1,\n",
    "'NV':2,\n",
    "'NY':1,\n",
    "'OH':1,\n",
    "'OK':2,\n",
    "'OR':0,\n",
    "'PA':1,\n",
    "'RI':0,\n",
    "'SC':0,\n",
    "'SD':2,\n",
    "'TN':2,\n",
    "'TX':0,\n",
    "'UT':2,\n",
    "'VA':1,\n",
    "'VT':1,\n",
    "'WA':0,\n",
    "'WI':1,\n",
    "'WV':2,\n",
    "'WY':2\n",
    "}\n",
    "\n",
    "oceanSwitcher = {\n",
    "'AL':0,\n",
    "'AR':0,\n",
    "'AZ':2,\n",
    "'CA':1,\n",
    "'CO':2,\n",
    "'CT':1,\n",
    "'DE':1,\n",
    "'FL':0,\n",
    "'GA':1,\n",
    "'HI':1,\n",
    "'IA':0,\n",
    "'ID':2,\n",
    "'IL':0,\n",
    "'IN':0,\n",
    "'KS':0,\n",
    "'KY':0,\n",
    "'LA':0,\n",
    "'MA':1,\n",
    "'MD':0,\n",
    "'ME':1,\n",
    "'MI':0,\n",
    "'MN':0,\n",
    "'MO':0,\n",
    "'MS':0,\n",
    "'MT':2,\n",
    "'NC':1,\n",
    "'ND':0,\n",
    "'NE':0,\n",
    "'NH':1,\n",
    "'NJ':0,\n",
    "'NM':2,\n",
    "'NV':2,\n",
    "'NY':1,\n",
    "'OH':1,\n",
    "'OK':0,\n",
    "'OR':1,\n",
    "'PA':1,\n",
    "'RI':0,\n",
    "'SC':0,\n",
    "'SD':0,\n",
    "'TN':1,\n",
    "'TX':0,\n",
    "'UT':2,\n",
    "'VA':0,\n",
    "'VT':1,\n",
    "'WA':2,\n",
    "'WI':0,\n",
    "'WV':1,\n",
    "'WY':2\n",
    "}\n",
    "\n",
    "for i in range(len(stateNump)):\n",
    "    mount[i] = mountSwitcher[stateNump[i]]\n",
    "    ocean[i] = oceanSwitcher[stateNump[i]]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(featSer)):\n",
    "    if prcp_days_tRay[i] == 'NaN' or prcp_days_tRay[i] == -9999 or prcp_days_tRay[i] == -8888 or prcp_days_tRay[\n",
    "        i] == -7777 or prcp_days_tRay[i] == -6666 or prcp_days_tRay[i] == -5555:\n",
    "        prcp_days_tRay[i] = prcpFreqM[featSer[i] - 1]\n",
    "\n",
    "df1 = df.copy()\n",
    "# Recreating filled dataset\n",
    "df = pd.DataFrame({'state': labels, 'mountains_prox' : mount, 'ocean_prox' : ocean, 'month' : featSer, 'prcp_days_tRay': prcp_days_tRay,\n",
    "                   \"temp_max_normalRay\": temp_max_normalRay, \"temp_min_normalRay\": temp_min_normalRay,\n",
    "                   \"snow_daysRay\": snow_daysRay, \"cloudsRay\": cloudsRay,\n",
    "                   \"dewRay\": dewRay, \"heatRay\": heatRay, \"windRay\": windRay})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#adds another column showing which bin every row falls into. The bins are based on the temp_max_normalRay column\n",
    "df['longitude_bin'] = pd.cut(df1['longitude'],\n",
    "                             bins=[-400, -125, -100, -75, np.inf],\n",
    "                             labels=[1, 2, 3, 4])\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2,\n",
    "                               random_state=42)  #n_splits is training date, test_size is target\n",
    "\n",
    "for train_index, test_index in split.split(df, df[\"longitude_bin\"]):  #shows it is an iterateable object\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]\n",
    "\n",
    "\n",
    "#SELECT COUNT(*) GROUP BY region;\n",
    "#     check if we need strat_sampling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lab_train = strat_train_set['state']\n",
    "feat_train = strat_train_set.drop(['state', 'longitude_bin'], axis=1)\n",
    "lab_test = strat_test_set['state']\n",
    "feat_test = strat_test_set.drop(['state', 'longitude_bin'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler = StandardScaler()  # makes 0 the average\n",
    "feat_train = scaler.fit_transform(feat_train)\n",
    "feat_train = scaler2.fit_transform(feat_train)\n",
    "feat_train += 1\n",
    "\n",
    "\"\"\"from sklearn.model_selection import train_test_split\n",
    "\n",
    "feat_train, feat_test, lab_train, lab_test = train_test_split(featPrepped, labels, random_state=42)\n",
    "# stratify by city for separating training and testing. stratified sampling\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feat_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from datetime import datetime as dt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "rc = RidgeClassifier()\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "\n",
    "rfParam = {\"max_depth\": range(5, 30, 5), \"min_samples_leaf\": range(1, 30, 10),\n",
    "           \"n_estimators\": range(1, 15, 4),\n",
    "           'bootstrap': [True,False],\n",
    "           'max_features': [1, 7, 'auto', 'sqrt'],\n",
    "           'warm_start': [True,False]}\n",
    "exParamOob = {\n",
    "    'n_estimators': range(1,15, 4),\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": range(1,30, 5),\n",
    "    'min_samples_split': range(2, 15, 4),\n",
    "    'min_samples_leaf': range(1, 25, 6),\n",
    "    'oob_score': [True, False],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', range(50, 200, 75)],\n",
    "    'bootstrap': [True],\n",
    "    'warm_start': [True, False],\n",
    "}\n",
    "exParam = {\n",
    "    'n_estimators': range(1,15, 4),\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\": range(1,30, 5),\n",
    "    'min_samples_split': range(2, 15, 4),\n",
    "    'min_samples_leaf': range(1, 25, 6),\n",
    "    'max_features': ['auto', 'sqrt', 'log2', range(50, 200, 75)],\n",
    "    'bootstrap': [True,False],\n",
    "    'warm_start': [True, False],\n",
    "}\n",
    "rnParam = {'radius': np.arange(0.8, 1.5, 0.4),\n",
    "           'weights': ['uniform', 'distance']}\n",
    "lrParam = {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], 'penalty': ['l2','none'],\n",
    "           'C': [10, 0.1, 0.001]}\n",
    "rcParam = {'alpha': [0.1, 0.5, 1.0]}\n",
    "svcParam = {'C': [0.01, 1, 3 'auto'],\n",
    "            'gamma': [0.1, 1.0, 2],\n",
    "            'kernel': ['rbf', 'linear', 'sigmoid']\n",
    "            }\n",
    "ldaParam = {'solver': ['svd', 'lsqr', 'eigen'], 'shrinkage': [np.arange(0, 1, 0.2), 'auto'],\n",
    "            'n_components': range(0, 5, 2), 'store_covariance': (True, False)}\n",
    "gpcParam = {}\n",
    "sgdParam = {'loss': ['hinge', 'log', 'modified_huber','squared_hinge', 'perceptron'],\n",
    "            'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "            'alpha': [0.0001, 0.01, 1, 75, 1000],\n",
    "            'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'], 'eta0': [1, 10, 100],\n",
    "            'n_iter': [1, 5, 10]}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#finished!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "mplN = MLPClassifier(batch_size='auto', warm_start=True, max_iter=400, activation = 'tanh')\n",
    "\n",
    "\n",
    "#ensemble_clf = [mplN,mplS,mplA]\n",
    "ensemble_clf = mplN\n",
    "\n",
    "mplNParam = {\n",
    "    'hidden_layer_sizes': [(60,80,100,120),(100,80,70,60)],\n",
    "    'alpha': (0.05,0.08,0.02,0.008),\n",
    "    'solver': ('lbfgs','adam'),\n",
    "}\n",
    "#(300,200,50,300),(400,200,150,100,60)\n",
    "\n",
    "parameters_list = mplNParam\n",
    "start = dt.now()\n",
    "\n",
    "Grid = HalvingGridSearchCV(estimator=ensemble_clf, param_grid=parameters_list,\n",
    "                                   n_jobs=-1, cv=3, verbose=0).fit(feat_train, lab_train)\n",
    "print(\"best param: \", Grid.best_params_)\n",
    "print(\"best score: \", Grid.best_score_)\n",
    "\n",
    "running_secs = (dt.now() - start).seconds\n",
    "print(\"Program running time: \", running_secs)\n",
    "\n",
    "\"\"\"\n",
    "tanh, 0.01, 120, lbfgs\n",
    "tanh, 0.1, 100, lbfgs      20\n",
    "tanh, 0.1, 100,50 , adam\n",
    "tan, .5, 200,100,200, lbfgs\n",
    "tan, 0.1, (60,300,100)adam\n",
    ".05, (300,200,50,300) adam    31%\n",
    "0.02, (60,80,100,120), adam\n",
    "\n",
    "final:\n",
    "mplN = MLPClassifier(batch_size='auto', warm_start=True, max_iter=400, activation = 'tanh', alpha = 0.02, solver = 'adam', hidden_layer_sizes=(60,80,100,120)\n",
    "\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best param:  {'activation': 'tanh', 'alpha': 5e-05, 'beta_1': 0.5, 'beta_2': 0.6, 'hidden_layer_sizes': (60, 80, 100, 120)}\n",
      "best score:  0.2981519086637096\n",
      "Program running time:  614\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\ntanh,1e-5,1=.5,2=.7,(150,100,50,10)\\ntan, 5e-6,1=.6,2=.7,(60,300,100)   26%\\n'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mplA = MLPClassifier(batch_size='auto', warm_start=True, activation = 'tanh', solver='adam', max_iter=400, early_stopping=True, beta_1=0.5)\n",
    "\n",
    "\n",
    "#ensemble_clf = [mplN,mplS,mplA]\n",
    "ensemble_clf = mplA\n",
    "\n",
    "mplAParam = {\n",
    "    'hidden_layer_sizes': [(60,80,100,120),(120,100,80,60)],\n",
    "    'alpha': ( 0.000001, 0.000005),\n",
    "    'beta_2': (0.4,0.5,0.6),\n",
    "}\n",
    "\n",
    "parameters_list = mplAParam\n",
    "start = dt.now()\n",
    "\n",
    "Grid = HalvingGridSearchCV(estimator=ensemble_clf, param_grid=parameters_list,\n",
    "                                   n_jobs=-1, cv=3, verbose=0).fit(feat_train, lab_train)\n",
    "print(\"best param: \", Grid.best_params_)\n",
    "print(\"best score: \", Grid.best_score_)\n",
    "\n",
    "running_secs = (dt.now() - start).seconds\n",
    "print(\"Program running time: \", running_secs)\n",
    "\n",
    "\"\"\"\n",
    "tanh,1e-5,1=.5,2=.7,(150,100,50,10)\n",
    "tan, 5e-6,1=.6,2=.7,(60,300,100)   26%\n",
    "tan, 5e-5, 1=.5, 2=.6, (60,80,100,120)    0.298       614 sec\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svcParam = {'C': [4, 2, 3],\n",
    "            'gamma': [0.1, 1.0, 2,'scale','auto'],\n",
    "            'kernel': ['rbf', 'linear', 'sigmoid']\n",
    "            }\n",
    "\n",
    "#ensemble_clf = [mplN,mplS,mplA]\n",
    "ensemble_clf = svc\n",
    "\n",
    "\n",
    "\n",
    "parameters_list = svcParam\n",
    "start = dt.now()\n",
    "\n",
    "Grid = HalvingGridSearchCV(estimator=ensemble_clf, param_grid=parameters_list,\n",
    "                                   n_jobs=-1, cv=3, verbose=0).fit(feat_train, lab_train)\n",
    "print(\"best param: \", Grid.best_params_)\n",
    "print(\"best score: \", Grid.best_score_)\n",
    "\n",
    "running_secs = (dt.now() - start).seconds\n",
    "print(\"Program running time: \", running_secs)\n",
    "\n",
    "\"\"\"\n",
    "3,2,rbf    25%\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#testing cell\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "ensemble_clf = rf\n",
    "\n",
    "\n",
    "rfParam = {\"max_depth\": range(12, 18, 2), \"min_samples_leaf\": range(1, 5, 2),\n",
    "           \"n_estimators\": range(9, 15, 2),\n",
    "           'bootstrap': [True,False],\n",
    "           'max_features': ['auto', 'sqrt'],\n",
    "           'warm_start': [True,False]}\n",
    "\n",
    "parameters_list = rfParam\n",
    "start = dt.now()\n",
    "\n",
    "Grid = HalvingGridSearchCV(estimator=ensemble_clf, param_grid=parameters_list,\n",
    "                                   n_jobs=-1, cv=3, verbose=0).fit(feat_train, lab_train)\n",
    "print(\"best param: \", Grid.best_params_)\n",
    "print(\"best score: \", Grid.best_score_)\n",
    "\n",
    "running_secs = (dt.now() - start).seconds\n",
    "print(\"Program running time: \", running_secs)\n",
    "\n",
    "\"\"\"\n",
    "rf {'bootstrap': False, 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 13, 'warm_start': False}    33%\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kn = KNeighborsClassifier()\n",
    "knParam = {\n",
    "    'n_neighbors': range(2, 30, 10),\n",
    "    'leaf_size': range(2, 40, 15),\n",
    "    'p': (1, 2),\n",
    "    'weights': ('uniform', 'distance'),\n",
    "    'metric': ('minkowski', 'chebyshev', 'euclidean', 'manhattan'),\n",
    "    'algorithm': ('auto', 'brute', 'kd_tree', 'ball_tree')}\n",
    "\n",
    "ensemble_clf = kn\n",
    "parameters_list = knParam\n",
    "start = dt.now()\n",
    "\n",
    "Grid = HalvingGridSearchCV(estimator=ensemble_clf, param_grid=parameters_list,\n",
    "                                   n_jobs=-1, cv=3, verbose=0).fit(feat_train, lab_train)\n",
    "print(\"best param: \", Grid.best_params_)\n",
    "print(\"best score: \", Grid.best_score_)\n",
    "\n",
    "running_secs = (dt.now() - start).seconds\n",
    "print(\"Program running time: \", running_secs)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=15,max_features='sqrt',min_samples_leaf=1,n_estimators=13)\n",
    "clf.fit(feat_train,lab_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kn = KNeighborsClassifier()\n",
    "knParam = {\n",
    "    'n_neighbors': range(2, 30, 10),\n",
    "    'leaf_size': range(2, 40, 15),\n",
    "    'p': (1, 2),\n",
    "    'weights': ('uniform', 'distance'),\n",
    "    'metric': ('minkowski', 'chebyshev', 'euclidean', 'manhattan'),\n",
    "    'algorithm': ('auto', 'brute', 'kd_tree', 'ball_tree')}\n",
    "\n",
    "ensemble_clf = kn\n",
    "parameters_list = knParam\n",
    "start = dt.now()\n",
    "\n",
    "Grid = HalvingGridSearchCV(estimator=ensemble_clf, param_grid=parameters_list,\n",
    "                                   n_jobs=-1, cv=3, verbose=0).fit(feat_train, lab_train)\n",
    "print(\"best param: \", Grid.best_params_)\n",
    "print(\"best score: \", Grid.best_score_)\n",
    "\n",
    "running_secs = (dt.now() - start).seconds\n",
    "print(\"Program running time: \", running_secs)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}